---
title: "hw5"
format: html
---
# Predicting ICU duration
Using the ICU cohort mimiciv_icu_cohort.rds you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient’s ICU stay will be longer than 2 days. You should use the los_long variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU intime, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay’s intime. For instance, last_careunit cannot be used in your algorithms.
## library
```{r}
sessionInfo()
```
```{r}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(lubridate)
library(miceRanger)
library(dplyr)
library(GGally)
library(gtsummary)
library(tidymodels)
library(yardstick)
library(dials)
library(kernlab)
library(ggthemes)
library(naniar)
library(kableExtra)
library(stacks)
library(vip)
```


## Data preprocessing and feature engineering.
```{r}
icu_cohort <- readRDS("../hw4/mimiciv_shiny/mimiciv_icu_cohort.rds") 
icu_cohort |>
  print(width = Inf)
```
check the missing value
```{r}
# check variables with more than 10000 missing values
icu_cohort %>%
  select_if(colSums(is.na(icu_cohort)) > 10000) %>%
  colnames()
# keep only variables with less than 10000 missing values
icu_cohort_discard <- icu_cohort %>%
  select_if(colSums(is.na(icu_cohort)) <= 10000) %>%
  print(width = Inf)
summary(icu_cohort_discard)
```
delete data collected after ICU intime
```{r}
icu_cohort_selected <- icu_cohort_discard |>
  select(-c("stay_id.y", "stay_id.x", "outtime", "dischtime", 
            "hospital_expire_flag", "last_careunit"))
print(icu_cohort_selected)
summary(icu_cohort_selected)
```
figure out missing value
```{r}
colMeans(is.na(icu_cohort_selected))
# make a function to replace outliers to `NA`s
winsorize <- function(x, lower=0.01, upper=0.99) {
  qnt <- quantile(x, probs = c(lower, upper), na.rm = TRUE)
  x[x < qnt[1]] <- qnt[1]
  x[x > qnt[2]] <- qnt[2]
  x
}
# replace the extrem data with NA
icu_cohort_replace <- icu_cohort_selected %>%
  mutate(across(c("HR", "NBPs", "NBPd", "RR", "BT", 
                  "Bicarbonate", "Chloride", "Creatinine", 
                  "Glucose", "Potassium", "Sodium",
                  "Hematocrit", "wbc"), winsorize))
summary(icu_cohort_replace)
class(icu_cohort_replace)
# delete all rows with NA in los_long, marital_status
icu_cohort_replace <- icu_cohort_replace |>
  filter(!is.na(los_long)) |>
  filter(!is.na(marital_status)) |>
  # also delete labevent NA rows
  filter(!if_all(c("Bicarbonate", "Chloride", "Creatinine", "Glucose", 
                   "Potassium", "Sodium", "Hematocrit", "wbc"), is.na)) |>
  print()

# delete unnessary columns
icu_cohort_replace <- icu_cohort_replace |>
  select(-c("admit_provider_id", "discharge_location")) |>
  print()


```
### keep cleaned data in a file
```{r}
# fill in the NA value
if (file.exists("icu_cohort_filled.rds")){
  icu_cohort_filled <- read_rds("icu_cohort_filled.rds")
}else{
  imputed_data <- miceRanger(icu_cohort_replace, 
                           m = 1,        
                           max.depth = 8,
                           num.trees = 50)  
  icu_cohort_filled <- completeData(imputed_data)
  icu_cohort_filled |>
    write_rds("icu_cohort_filled.rds")
}
icu_cohort_model <- as.data.frame(icu_cohort_filled) |>
  rename_with(~ gsub("^Dataset_1\\.", "", .x)) |>
  # Keep necessary columns
  select(
    "los_long",  
    "gender", "marital_status", "race", "first_careunit",
    "Bicarbonate", "Chloride", "Creatinine", "Glucose", 
    "Potassium", "Sodium", "Hematocrit", "wbc",
    "HR", "NBPs", "NBPd", "RR", "BT",
    "age_intime", "subject_id", "stay_id", "hadm_id"
  ) |>
  mutate(los_long = factor(los_long, levels = c(FALSE, TRUE))) |>
  mutate(age_intime = as.numeric(age_intime)) |>
  print()
```
print the summary table
```{r}
summary_table <- icu_cohort_model %>% 
  select("los_long",  
    "gender", "marital_status", "race", "first_careunit",
    "Bicarbonate", "Chloride", "Creatinine", "Glucose", 
    "Potassium", "Sodium", "Hematocrit", "wbc",
    "HR", "NBPs", "NBPd", "RR", "BT") |>
  tbl_summary(by = los_long)
summary_table
```

## split the dataset
Partition data into 50% training set and 50% test set. Stratify partitioning according to los_long. For grading purpose, sort the data by subject_id, hadm_id, and stay_id and use the seed 203 for the initial data split. Below is the sample code.
```{r}
set.seed(203)
# arrange the data
icu_cohort_model_split <- icu_cohort_model |>
  arrange(subject_id, hadm_id, stay_id) |>
  select(-c("subject_id",
            "hadm_id",
            "stay_id"))
data_split <- initial_split(
  icu_cohort_model,
  strata = "los_long",
  prop = 0.5
)
data_split
icu_cohort_train <- training(data_split)
dim(icu_cohort_train)
icu_cohort_test <- testing(data_split)
dim(icu_cohort_test)
```
Set cross-validation partitions.
```{r}
set.seed(203)
folds <- vfold_cv(icu_cohort_train, v = 5)
folds
```
## Logistic Regression
### Recipe
```{r}
logit_recipe <-
  recipe(
    los_long ~ .,
    data = icu_cohort_train
  ) |>
  # create traditional dummy variables
  step_dummy(all_nominal_predictors()) |>
  # zero-variance filter
  step_zv(all_numeric_predictors()) |> 
  # center and scale numeric data
  step_normalize(all_numeric_predictors()) |>
  # estimate the means and standard deviations
  # prep(training = Heart_other, retain = TRUE) |>
  print()
```
### Model
```{r}
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) |> 
  set_engine("glmnet", standardize = FALSE) |>
  print()
```
### Workflow
```{r}
logit_wf <- workflow() |>
  add_recipe(logit_recipe) |>
  add_model(logit_mod) |>
  print()
```
### Tuning grid
```{r}
param_grid <- grid_regular(
  penalty(range = c(-6, 3)),  
  mixture(),
  levels = c(100, 5)            
  ) |>
  print()
```

### Fit cross-validation
```{r}
(logit_fit <- logit_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy),
    control = control_stack_grid()
    )) 
logit_fit
```
### Visualize CV results:
```{r}
logit_fit |>
  # aggregate metrics from K folds
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = penalty, y = mean, color = factor(mixture))) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()
show_notes(logit_fit)
```
Show the top 5 models.
```{r}
logit_fit |>
  show_best(metric = "roc_auc") |>
  print()
```
select the best model.
```{r}
best_logit <- logit_fit |>
  select_best(metric = "roc_auc")
best_logit
```
### Finalize the model
```{r}
# Final workflow
final_logit_wf <- logit_wf |>
  finalize_workflow(best_logit)
final_logit_wf
# Fit the whole training set, then predict the test cases
final_logit_fit <- 
  final_logit_wf |>
  last_fit(data_split)
final_logit_fit
# Test metrics
final_logit_fit |> 
  collect_metrics()
```
### Predict the los
```{r}
# 提取测试集
test_data <- testing(data_split)
# 提取模型
final_model <- extract_fit_parsnip(final_logit_fit)
# 预测概率
# 使用 final_fit 直接预测（推荐）
prob_pred <- final_logit_fit %>% collect_predictions()

colnames(prob_pred)
# 将概率转换为二分类（0.5 作为阈值）
predict_class <- ifelse(prob_pred$.pred_TRUE > 0.5, TRUE, FALSE)
# 计算混淆矩阵
conf_matrix <- table(observed = test_data$los_long, predicted = predict_class)
print(conf_matrix)
accuracy <- (15301 + 10129) / (15301 + 8753 + 13040 + 10129)
print(paste("Accuracy:", round(accuracy, 4)))
sensitivity <- 10129 / (10129 + 13040)
print(paste("Sensitivity:", round(sensitivity, 4)))
specificity <- 15301 / (15301 + 8753)
print(paste("Specificity:", round(specificity, 4)))

```

## Random forest
### Recipe
```{r}
rf_recipe <- 
  recipe(
    los_long ~ ., 
    data = icu_cohort_train
  ) |>
  # # create traditional dummy variables (not necessary for random forest in R)
  # step_dummy(all_nominal()) |>
  # zero-variance filter
  step_zv(all_numeric_predictors()) |>
  # # center and scale numeric data (not necessary for random forest)
  # step_normalize(all_numeric_predictors()) |>
  # estimate the means and standard deviations
  # prep(training = Heart_other, retain = TRUE) |>
  step_nzv(all_predictors()) |>
  print()
```
### Model
```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    # Number of predictors randomly sampled in each split
    mtry = tune(),
    # Number of trees in ensemble
    trees = tune()
  ) |> 
  set_engine(
    "ranger", importance = "impurity"
  )
rf_mod
```

### Tuning grid
```{r}
param_grid <- grid_regular(
  trees(range = c(100L, 400L)), 
  mtry(range = c(1L, 6L)),
  levels = c(5, 5)
  )
param_grid
```
workflow
```{r}
rf_wf <- workflow() |>
  add_recipe(rf_recipe) |>
  add_model(rf_mod)
rf_wf
```
### Fit cross-validation
```{r}
rm(accuracy)
rf_fit <- rf_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy),
    control = control_stack_grid()
    )
rf_fit
```
### Visualize CV results
```{r}
rf_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = trees, y = mean, color = factor(mtry))) +
  geom_point() + 
  # geom_line() + 
  labs(x = "Num. of Trees", y = "CV AUC")
```
Show the top 5 models.
```{r}
rf_fit |>
  show_best(metric = "roc_auc")

# Let’s select the best model.
best_rf <- rf_fit |>
  select_best(metric = "roc_auc")
best_rf
```
### Finalize the model
```{r}
# Final workflow
final_rf_wf <- rf_wf |>
  finalize_workflow(best_rf)
final_rf_wf
# Fit the whole training set, then predict the test cases
final_rf_fit <- 
  final_rf_wf |>
  last_fit(data_split)
final_rf_fit
# Test metrics
final_rf_fit |> 
  collect_metrics()
```
check features
```{r}
library(vip)
final_rf_fit |> 
  extract_fit_parsnip() |> 
  vip(num_features = 18)  # 查看所有特征的重要性
```

## XGBoost
### Recipe
```{r}
gb_recipe <- 
  recipe(
    los_long ~ ., 
    data = icu_cohort_train
  ) |>
  # create traditional dummy variables (necessary for xgboost)
  step_dummy(all_nominal_predictors()) |>
  # zero-variance filter
  step_zv(all_numeric_predictors()) |> 
  # estimate the means and standard deviations
  # prep(training = Heart_other, retain = TRUE) |>
  print()
```
### Model
```{r}
gb_mod <- 
  boost_tree(
    mode = "classification",
    trees = 200, 
    tree_depth = tune(),
    learn_rate = tune()
  ) |> 
  set_engine("xgboost")
gb_mod
```
### Tuning grid
```{r}
param_grid <- grid_regular(
  tree_depth(range = c(1L, 6L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 10)
  )
param_grid
```
workflow
```{r}
gb_wf <- workflow() |>
  add_recipe(gb_recipe) |>
  add_model(gb_mod)
gb_wf
```
### Fit cross-validation.
```{r}
gb_fit <- gb_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy),
    control = control_stack_grid()
    )
gb_fit
```
### Visualize CV results
```{r}
gb_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
# Show the top 5 models.
gb_fit |>
  show_best(metric = "roc_auc")
# Let’s select the best model.
best_gb <- gb_fit |>
  select_best(metric = "roc_auc")
best_gb
```
### Finalize our model
```{r}
# Final workflow
final_gb_wf <- gb_wf |>
  finalize_workflow(best_gb)
final_gb_wf
# Fit the whole training set, then predict the test cases
final_gb_fit <- 
  final_gb_wf |>
  last_fit(data_split)
final_gb_fit
# Test metrics
final_gb_fit |> 
  collect_metrics()
```
## Model Stacking
```{r}
mimic_model_st <-
  stacks() %>%
  add_candidates(logit_fit) %>%
  add_candidates(rf_fit) %>%
  add_candidates(gb_fit) %>%
  blend_predictions(
    penalty = 10^(-6:2),
    metrics = c("roc_auc", "accuracy")
  ) %>%
  fit_members()
autoplot(mimic_model_st)
autoplot(mimic_model_st, type = "weights")
```
### Predict
```{r}
mimic_pred <- icu_cohort_test %>%
  bind_cols(predict(mimic_model_st, ., type = "prob"))
yardstick::roc_auc(
  mimic_pred,
  truth = los_long,
  contains(".pred_False")
)
```
### Model Performance
```{r}
final_log <- extract_workflow(final_log_fit)
final_log %>% 
  extract_fit_parsnip() %>% 
  vip()
final_tree <- extract_workflow(final_gb_fit)
final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
final_rf <- extract_workflow(final_rf_fit)
final_rf %>% 
  extract_fit_parsnip() %>% 
  vip()
```



