---
title: "hw5"
format: 
  html:
    toc: true                
    toc-depth: 3             
    toc-location: left       
    number-sections: true    
---
# Predicting ICU duration
Using the ICU cohort mimiciv_icu_cohort.rds you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient’s ICU stay will be longer than 2 days. You should use the los_long variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU intime, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay’s intime. For instance, last_careunit cannot be used in your algorithms.

## library
```{r}
sessionInfo()
```
```{r}
library(tidyverse)
library(ggplot2)
library(corrplot)
library(lubridate)
library(miceRanger)
library(dplyr)
library(GGally)
library(gtsummary)
library(tidymodels)
library(yardstick)
library(dials)
library(kernlab)
library(ggthemes)
library(naniar)
library(kableExtra)
library(stacks)
library(vip)
library(h2o)
h2o.init(nthreads = -1, max_mem_size = "8G")
```


## Data preprocessing and feature engineering.
```{r}
icu_cohort <- readRDS("../hw4/mimiciv_shiny/mimiciv_icu_cohort.rds") 
icu_cohort |>
  print(width = Inf)
```
check the missing value
```{r}
# check variables with more than 10000 missing values
icu_cohort %>%
  select_if(colSums(is.na(icu_cohort)) > 10000) %>%
  colnames()
# keep only variables with less than 10000 missing values
icu_cohort_discard <- icu_cohort %>%
  select_if(colSums(is.na(icu_cohort)) <= 10000) 
```
delete data collected after ICU intime
```{r}
icu_cohort_selected <- icu_cohort_discard |>
  select(-c("stay_id.y", "stay_id.x", "outtime", "dischtime", 
            "hospital_expire_flag", "last_careunit"))
print(icu_cohort_selected)
```
figure out missing value
```{r}
# make a function to replace outliers to `NA`s
winsorize <- function(x, lower=0.01, upper=0.99) {
  qnt <- quantile(x, probs = c(lower, upper), na.rm = TRUE)
  x[x < qnt[1]] <- qnt[1]
  x[x > qnt[2]] <- qnt[2]
  x
}
# replace the extrem data with NA
icu_cohort_replace <- icu_cohort_selected %>%
  mutate(across(c("HR", "NBPs", "NBPd", "RR", "BT", 
                  "Bicarbonate", "Chloride", "Creatinine", 
                  "Glucose", "Potassium", "Sodium",
                  "Hematocrit", "wbc"), winsorize))
# delete all rows with NA in los_long, marital_status
icu_cohort_replace <- icu_cohort_replace |>
  filter(!is.na(los_long)) |>
  filter(!is.na(marital_status)) |>
  # also delete labevent NA rows
  filter(!if_all(c("Bicarbonate", "Chloride", "Creatinine", "Glucose", 
                   "Potassium", "Sodium", "Hematocrit", "wbc"), is.na)) 
# delete unnessary columns
icu_cohort_replace <- icu_cohort_replace |>
  select(-c("admit_provider_id", "discharge_location")) |>
  print()


```
### keep cleaned data in a file
```{r}
# fill in the NA value
if (file.exists("icu_cohort_filled.rds")){
  icu_cohort_filled <- read_rds("icu_cohort_filled.rds")
}else{
  imputed_data <- miceRanger(icu_cohort_replace, 
                           m = 1,        
                           max.depth = 8,
                           num.trees = 50)  
  icu_cohort_filled <- completeData(imputed_data)
  icu_cohort_filled |>
    write_rds("icu_cohort_filled.rds")
}
icu_cohort_model <- as.data.frame(icu_cohort_filled) |>
  rename_with(~ gsub("^Dataset_1\\.", "", .x)) |>
  # Keep necessary columns
  select(
    "los_long",  
    "gender", "marital_status", "race", "first_careunit",
    "Bicarbonate", "Chloride", "Creatinine", "Glucose", 
    "Potassium", "Sodium", "Hematocrit", "wbc",
    "HR", "NBPs", "NBPd", "RR", "BT",
    "age_intime", "subject_id", "stay_id", "hadm_id"
  ) |>
  mutate(los_long = factor(los_long, levels = c(FALSE, TRUE))) |>
  mutate(age_intime = as.numeric(age_intime)) |>
  mutate(gender = as.factor(gender)) |>
  mutate(marital_status = as.factor(marital_status)) |>
  mutate(race = as.factor(race)) 
```
print the summary table
```{r}
summary_table <- icu_cohort_model %>% 
  select("los_long",  
    "gender", "marital_status", "race", "first_careunit",
    "Bicarbonate", "Chloride", "Creatinine", "Glucose", 
    "Potassium", "Sodium", "Hematocrit", "wbc",
    "HR", "NBPs", "NBPd", "RR", "BT") |>
  tbl_summary(by = los_long)
summary_table
```

## split the dataset
Partition data into 50% training set and 50% test set. Stratify partitioning according to los_long. For grading purpose, sort the data by subject_id, hadm_id, and stay_id and use the seed 203 for the initial data split. Below is the sample code.
```{r}
set.seed(203)
# arrange the data
icu_cohort_model_split <- icu_cohort_model |>
  arrange(subject_id, hadm_id, stay_id) |>
  select(-c("subject_id",
            "hadm_id",
            "stay_id"))
data_split <- initial_split(
  icu_cohort_model_split,
  strata = "los_long",
  prop = 0.5
)
data_split
icu_cohort_train <- training(data_split)
dim(icu_cohort_train)
icu_cohort_test <- testing(data_split)
dim(icu_cohort_test)
icu_cohort_train <- as.h2o(icu_cohort_train)
icu_cohort_test <- as.h2o(icu_cohort_test)
```
## Logistic Regression
### Model
```{r}
logit_mod_h2o <- h2o.glm(
  x = c("gender", "marital_status", "race", "first_careunit",
        "Bicarbonate", "Chloride", "Creatinine", "Glucose", 
        "Potassium", "Sodium", "Hematocrit", "wbc",
        "HR", "NBPs", "NBPd", "RR", "BT", "age_intime"),
  y = "los_long",
  training_frame = icu_cohort_train,
  family = "binomial",
  alpha = 0.5,
  lambda_search = TRUE,
  nfolds = 5,             
  keep_cross_validation_predictions = TRUE
)
```

### Visualize CV results:
```{r}
h2o.varimp_plot(logit_mod_h2o)  
h2o.performance(logit_mod_h2o)  

```


## Random forest
### Model
```{r}
rf_mod_h2o <- h2o.randomForest(
  x = c("gender", "marital_status", "race", "first_careunit",
        "Bicarbonate", "Chloride", "Creatinine", "Glucose", 
        "Potassium", "Sodium", "Hematocrit", "wbc",
        "HR", "NBPs", "NBPd", "RR", "BT", "age_intime"),  
  y = "los_long",         
  training_frame = icu_cohort_train,  
  ntrees = 100,           
  mtries = -1,           
  max_depth = 20,         
  min_rows = 5,          
  seed = 1234,           
  nfolds = 5,
  balance_classes = TRUE, 
  keep_cross_validation_predictions = TRUE
)

```
### Visualize CV results
```{r}
h2o.varimp_plot(rf_mod_h2o)  
```
## XGBoost
### Model
```{r}
gb_mod_h2o <- h2o.gbm(
  x = c("gender", "marital_status", "race", "first_careunit",
        "Bicarbonate", "Chloride", "Creatinine", "Glucose", 
        "Potassium", "Sodium", "Hematocrit", "wbc",
        "HR", "NBPs", "NBPd", "RR", "BT", "age_intime"),  
  y = "los_long",         
  training_frame = icu_cohort_train,  
  ntrees = 200,           
  max_depth = 6,          
  learn_rate = 0.05,      
  sample_rate = 0.8,      
  col_sample_rate = 0.8,  
  seed = 1234,            
  nfolds = 5,
  keep_cross_validation_predictions = TRUE
)

```


### Visualize CV results
```{r}
h2o.varimp_plot(gb_mod_h2o)  
```
## Model Stacking
```{r}
feature_columns <- setdiff(names(icu_cohort_train), "los_long")

h2o_stacked_model <- h2o.stackedEnsemble(
  x = feature_columns,
  y = "los_long",
  training_frame = icu_cohort_train,
  base_models = list(rf_mod_h2o, gb_mod_h2o, logit_mod_h2o)  
)
```

### Predict
```{r}
h2o_stacked_predictions <- h2o.predict(h2o_stacked_model, icu_cohort_test) |>
  print()
```
### Model Performance
```{r}
library(pROC)
h2o.performance(h2o_stacked_model, newdata = icu_cohort_test)
# stacking model
perf_stack <- h2o.performance(h2o_stacked_model, newdata = icu_cohort_test)
auc_stack <- h2o.auc(perf_stack)

# rf model
perf_rf <- h2o.performance(rf_mod_h2o, newdata = icu_cohort_test)
auc_rf <- h2o.auc(perf_rf)

# gb model
perf_gb <- h2o.performance(gb_mod_h2o, newdata = icu_cohort_test)
auc_gb <- h2o.auc(perf_gb)

# logit model
perf_logit <- h2o.performance(logit_mod_h2o, newdata = icu_cohort_test)
auc_logit <- h2o.auc(perf_logit)

# table
auc_table <- data.frame(
  Model = c("Stacked Ensemble", "Random Forest", 
            "GBM", "Logistic Regression"),
  AUC = c(auc_stack, auc_rf, auc_gb, auc_logit)
) |>
  print()
```
**conclusion**

The table above compares the AUC (Area Under the ROC Curve) values for four different classification models—Stacked Ensemble, Random Forest, GBM (Gradient Boosting Machine), and Logistic Regression. The Stacked Ensemble achieves the highest AUC at 0.6370, followed by GBM at 0.6342, Random Forest at 0.6264, and Logistic Regression at 0.6031.The stacked model slightly outperforms the individual algorithms.

### Predicting compare
```{r}
# Logistic Regression
perf_logit <- h2o.performance(logit_mod_h2o, newdata = icu_cohort_test)
cm_logit <- h2o.confusionMatrix(perf_logit, thresholds = 0.5)
cat("\n--- Logistic Regression Confusion Matrix (threshold=0.5) ---\n")
print(cm_logit)

# Random Forest
perf_rf <- h2o.performance(rf_mod_h2o, newdata = icu_cohort_test)
cm_rf <- h2o.confusionMatrix(perf_rf, thresholds = 0.5)
cat("\n--- Random Forest Confusion Matrix (threshold=0.5) ---\n")
print(cm_rf)

# GBM
perf_gb <- h2o.performance(gb_mod_h2o, newdata = icu_cohort_test)
cm_gb <- h2o.confusionMatrix(perf_gb, thresholds = 0.5)
cat("\n--- GBM Confusion Matrix (threshold=0.5) ---\n")
print(cm_gb)

# Stacked Ensemble
perf_stack <- h2o.performance(h2o_stacked_model, newdata = icu_cohort_test)
cm_stack <- h2o.confusionMatrix(perf_stack, thresholds = 0.5)
cat("\n--- Stacked Ensemble Confusion Matrix (threshold=0.5) ---\n")
print(cm_stack)

```
**conclusion**

The Stacked Ensemble achieves the lowest overall error and the highest AUC, indicating that combining multiple base learners yields better predictive performance. Random Forest and GBM both outperform Logistic Regression, reflecting the benefit of more flexible, non-linear modeling. However, each model still shows substantial difficulty correctly identifying positive cases, as evidenced by higher error rates in the TRUE class. Overall, these results suggest that ensemble methods—especially stacking—can provide incremental yet meaningful improvements over individual algorithms.

### ROC plot
```{r}
true_labels <- as.vector(icu_cohort_test$los_long)
get_roc_df <- function(model, test_frame, true_labels, pos_class = "TRUE") {
  preds <- h2o.predict(model, test_frame)
  pred_prob <- as.vector(preds[[pos_class]])
  # use pROC
  roc_obj <- roc(true_labels, pred_prob)
  roc_df <- data.frame(
    fpr = 1 - rev(roc_obj$specificities),
    tpr = rev(roc_obj$sensitivities)
  )
  return(roc_df)
}

# get ROC for each model
roc_logit <- get_roc_df(logit_mod_h2o, icu_cohort_test, true_labels)
roc_rf    <- get_roc_df(rf_mod_h2o, icu_cohort_test, true_labels)
roc_gb    <- get_roc_df(gb_mod_h2o, icu_cohort_test, true_labels)
roc_stack <- get_roc_df(h2o_stacked_model, icu_cohort_test, true_labels)

roc_logit$model <- "Logistic Regression"
roc_rf$model    <- "Random Forest"
roc_gb$model    <- "GBM"
roc_stack$model <- "Stacked Ensemble"

# merge all ROC data
roc_data <- rbind(roc_logit, roc_rf, roc_gb, roc_stack)

# ggplot
ggplot(roc_data, aes(x = fpr, y = tpr, color = model)) +
  geom_line(size = 1) +
  geom_abline(linetype = "dashed", color = "black") +
  labs(title = "ROC Curves for Different H2O Models",
       x = "False Positive Rate (FPR)",
       y = "True Positive Rate (TPR)") +
  theme_minimal()


```
**conclusion**

From the figure it can be seen that stacking model has the best ROC curve performance, which is similar to that of GBM, random forests are slightly inferior to them, and logistic regression has the worst ROC performance.

**What are the most important features in predicting long ICU stays? How do the models compare in terms of performance and interpretability?**

From the feature importance analysis, we can see all three models agree that first careunit is one of the most important features in predicting long ICU stays.The last lab measurements before the ICU stay and first vital measurements during ICU stay are also in top important features. This suggests that the last lab measurements before the ICU stay and first vital measurements during ICU stay are important indicators of the patient’s condition and may be useful in predicting long ICU stays.
In this case, the trade-off between performance and interpretability is clear. While the model stacking approach gives the best ROC AUC, it does so at the cost of interpretability. On the other hand, logistic regression offers ease of interpretation but doesn’t perform as well. The gradient boosting model presents a good balance, with relatively high performance metrics and a degree of interpretability through feature importance scores.



